{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extreme-boost",
   "metadata": {},
   "source": [
    "### **D2APR: Aprendizado de M√°quina e Reconhecimento de Padr√µes** (IFSP, Campinas) <br/>\n",
    "**Prof**: Samuel Martins (Samuka) <br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-memorabilia",
   "metadata": {},
   "source": [
    "### Custom CSS style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-professor",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dashed-box {\n",
       "    border: 1px dashed black !important;\n",
       "}\n",
       ".dashed-box tr {\n",
       "  background-color: white !important;  \n",
       "}\n",
       ".alt-tab {\n",
       "    background-color: black;\n",
       "    color: #ffc351;\n",
       "    padding: 4px;\n",
       "    font-size: 1em;\n",
       "    font-weight: bold;\n",
       "    font-family: monospace;\n",
       "}\n",
       "// add your CSS styling here\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".dashed-box {\n",
    "    border: 1px dashed black !important;\n",
    "}\n",
    ".dashed-box tr {\n",
    "  background-color: white !important;  \n",
    "}\n",
    ".alt-tab {\n",
    "    background-color: black;\n",
    "    color: #ffc351;\n",
    "    padding: 4px;\n",
    "    font-size: 1em;\n",
    "    font-weight: bold;\n",
    "    font-family: monospace;\n",
    "}\n",
    "// add your CSS styling here\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-sound",
   "metadata": {},
   "source": [
    "<span style='font-size: 2.5em'><b>California Housing üè°</b></span><br/>\n",
    "<span style='font-size: 1.5em'>Predict the median housing price in California districts</span>\n",
    "\n",
    "<span style=\"background-color: #ffc351; padding: 4px; font-size: 1em;\"><b>Sprint 6</b></span>\n",
    "\n",
    "<img src=\"./imgs/california-flag.png\" width=300/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-copying",
   "metadata": {},
   "source": [
    "## Before starting this notebook\n",
    "This jupyter notebook is designed for **experimental and teaching purposes**. <br/>\n",
    "Although it is (relatively) well organized, it aims at solving the _target problem_ by evaluating (and documenting) _different solutions_ for somes steps of the **machine learning pipeline** ‚Äî see the ***Machine Learning Project Checklist by xavecoding***. <br/>\n",
    "We tried to make this notebook as literally a _notebook_. Thus, it contains notes, drafts, comments, etc.<br/>\n",
    "\n",
    "For teaching purposes, some parts of the notebook may be _overcommented_. Moreover, to simulate a real development scenario, we will divide our solution and experiments into **\"sprints\"** in which each sprint has some goals (e.g., perform _feature selection_, train more ML models, ...). <br/>\n",
    "The **sprint goal** will be stated at the beginning of the notebook.\n",
    "\n",
    "A ***final notebook*** (or any other kind of presentation) that compiles and summarizes all sprints ‚Äî the target problem, solutions, and findings ‚Äî should be created later.\n",
    "\n",
    "#### Conventions\n",
    "\n",
    "<ul>\n",
    "    <li>üí° indicates a tip. </li>\n",
    "    <li> ‚ö†Ô∏è indicates a warning message. </li>\n",
    "    <li><span class='alt-tab'>alt tab</span> indicates and an extra content (<i>e.g.</i>, slides) to explain a given concept.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-portuguese",
   "metadata": {},
   "source": [
    "## üéØ Sprint Goals\n",
    "- Fine-tune the _hyperparameters_ of the Polynomial Regression models form Sprint #5.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-perfume",
   "metadata": {},
   "source": [
    "### 0. Imports and default settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-gateway",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-acquisition",
   "metadata": {},
   "source": [
    "We will consider the same two scenarios for **Polynomial Regression** from Sprint #5 in this sprint:\n",
    "1. Use _only_ the `median_income`.\n",
    "2. Use _all features_ except those that generated the aggregate features (`total_rooms`, `total_bedrooms`, `population`, `household`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-powder",
   "metadata": {},
   "source": [
    "### 5.1. Load the cleaned training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-synthetic",
   "metadata": {},
   "source": [
    "Let's consider the training and testing sets already cleaned (sprint #2):\n",
    "- Drop duplicated instances (no found)\n",
    "- Drop instances with `housing_median_age` capped at 52\n",
    "- Drop instances with `median_house_value` capped at 500001.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned training set\n",
    "housing_train = pd.read_csv('./datasets/housing_train_sprint-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-delight",
   "metadata": {},
   "source": [
    "### 5.2. Separate the _features_ and the _target outcome_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the target outcome into a numpy array\n",
    "y_train = housing_train['median_house_value'].values\n",
    "\n",
    "# overwrite the dataframe with only the features  \n",
    "housing_train = housing_train.drop(columns=['median_house_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-joshua",
   "metadata": {},
   "source": [
    "### 5.3. Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-burton",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let's include the **Polynomial Regression** objects (`PolynomialFeatures()` + `LinearRegression()`) into our **pipeline**. So, it is no longer _just_ dedicated to preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-pepper",
   "metadata": {},
   "source": [
    "#### **Scenario 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "attributes_scenario_1 = ['median_income']\n",
    "\n",
    "pipeline_scenario_1 = Pipeline([\n",
    "    ('imputer', SimpleImputer()),   # let's evaluate the mean and median inputation\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('poly', PolynomialFeatures())\n",
    "])\n",
    "\n",
    "# we will just use the ColumnTransformer because it automaticaly filters the required columns for us before performing the pipeline.\n",
    "# (name, transformer, columns)\n",
    "preprocessed_pipeline_scenario_1 = ColumnTransformer([\n",
    "    (\"numerical\", pipeline_scenario_1, attributes_scenario_1)\n",
    "])\n",
    "\n",
    "\n",
    "# full pipeline: preprocessing + model training/prediction\n",
    "full_pipeline_scenario_1 = Pipeline([\n",
    "        ('preprocessing', preprocessed_pipeline_scenario_1),\n",
    "        ('lin_regression', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-venice",
   "metadata": {},
   "source": [
    "#### **Scenario 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### feature engineering method from the Sprint #4\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our 3 new features are based on some the features: totalrooms, \n",
    "# column index\n",
    "rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx = 3, 4, 5, 6\n",
    "\n",
    "class HousingFeatEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        n_rows = X.shape[0]\n",
    "        \n",
    "        # creating the new features\n",
    "        rooms_per_household = X[:, rooms_col_idx] / X[:, households_col_idx]\n",
    "        population_per_household = X[:, population_col_idx] / X[:, households_col_idx]\n",
    "        bedrooms_per_room = X[:, bedrooms_col_idx] / X[:, rooms_col_idx]\n",
    "                \n",
    "        # to concatenate the new array as columns in our feature matrix, we need to reshape them first\n",
    "        rooms_per_household = rooms_per_household.reshape((n_rows, 1))\n",
    "        population_per_household = population_per_household.reshape((n_rows, 1))\n",
    "        bedrooms_per_room = bedrooms_per_room.reshape((n_rows, 1))\n",
    "        \n",
    "        # concatenating the new features into the feature matrix\n",
    "        X_out = np.hstack((X, rooms_per_household, population_per_household, bedrooms_per_room))\n",
    "        \n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our 3 new features are based on some the features: totalrooms, \n",
    "# column index\n",
    "rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx = 3, 4, 5, 6\n",
    "\n",
    "class DropFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = np.delete(X, [rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx], axis=1)\n",
    "        \n",
    "        # for debugging\n",
    "        if self.verbose:\n",
    "            np.set_printoptions(suppress=True)\n",
    "            print('X[:5]')\n",
    "            print(X[:5])\n",
    "            print('\\nX_out[:5]')\n",
    "            print(X_out[:5])\n",
    "        \n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "attributes_scenario_2 = housing_train.columns.drop('ocean_proximity')\n",
    "\n",
    "pipeline_scenario_2 = Pipeline([\n",
    "    ('imputer', SimpleImputer()),  # let's evaluate the mean and median inputation\n",
    "    ('feat_engineering', HousingFeatEngineering()),\n",
    "    ('drop_features', DropFeatures(verbose=False)),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('poly', PolynomialFeatures())\n",
    "])\n",
    "\n",
    "# we will just use the ColumnTransformer because it automaticaly filters the required columns for us before performing the pipeline.\n",
    "# (name, transformer, columns)\n",
    "preprocessed_pipeline_scenario_2 = ColumnTransformer([\n",
    "    (\"numerical\", pipeline_scenario_2, attributes_scenario_2)\n",
    "])\n",
    "\n",
    "# full pipeline: preprocessing + model training/prediction\n",
    "full_pipeline_scenario_2 = Pipeline([\n",
    "        ('preprocessing', preprocessed_pipeline_scenario_2),\n",
    "        ('lin_regression', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-marina",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è 6. Train ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-stake",
   "metadata": {},
   "source": [
    "### 6.1. Hyperparameter Optimization (fine-tuning)\n",
    "\n",
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td><span class='alt-tab'>alt tab</span></td>\n",
    "    <td><b>Slides:</b> Hyperparameter Optimization (fine-tuning).</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-friday",
   "metadata": {},
   "source": [
    "### **Scenario 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-stomach",
   "metadata": {},
   "source": [
    "#### **Finding out the hyperparameter key names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_scenario_1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-browse",
   "metadata": {},
   "source": [
    "#### **Grid-search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-upset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stone-dimension",
   "metadata": {},
   "source": [
    "### **Scenario 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_scenario_2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-carroll",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "canadian-workplace",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-stewart",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
